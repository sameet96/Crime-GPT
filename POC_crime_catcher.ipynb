{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Audio\n",
    "\n",
    "import cv2  # We're using OpenCV to read video, to install !pip install opencv-python\n",
    "import base64\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Now you can access the environment variables\n",
    "api_key = os.getenv('OPEN_AI_KEY')\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person detected: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLO model\n",
    "net = cv2.dnn.readNetFromDarknet('/Users/sameet/Crime GPT/yolov3/yolov3.cfg', '/Users/sameet/Crime GPT/yolov3/yolov3.weights')\n",
    "\n",
    "# Get the output layer names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Open the video file\n",
    "video = cv2.VideoCapture(\"/Users/sameet/Crime GPT/Anomaly-Videos-Part-1/Assault/Assault001_x264.mp4\")\n",
    "\n",
    "# Flag to indicate if a person is detected\n",
    "person_detected = False\n",
    "\n",
    "# Loop through the frames of the video\n",
    "while True:\n",
    "    # Read the frame\n",
    "    ret, frame = video.read()\n",
    "    if not ret or person_detected:\n",
    "        break\n",
    "    \n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, None, fx=0.4, fy=0.4)\n",
    "    \n",
    "    # Convert the frame to a blob\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    \n",
    "    # Set the input to the network\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # Process the outputs\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:  # Person class\n",
    "                person_detected = True  # Set the flag to True if a person is detected\n",
    "                # Get the coordinates of the bounding box\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                width = int(detection[2] * frame.shape[1])\n",
    "                height = int(detection[3] * frame.shape[0])\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                \n",
    "                # Add the bounding box information to the lists\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "                break  # Exit the loop as a person is detected\n",
    "    \n",
    "    if person_detected:\n",
    "        break  # Exit the main loop as a person is detected\n",
    "    \n",
    "    # Apply non-maximum suppression to remove overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    # Draw the bounding boxes on the frame\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        left, top, width, height = box\n",
    "        cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Person Detection', frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the windows\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Return the flag\n",
    "print(f\"Person detected: {person_detected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 frames read.\n",
      "Based on the frames provided, it appears the video depicts a physical altercation involving multiple individuals. This type of scene can be categorized under crime, specifically assault.\n",
      "\n",
      "### Description:\n",
      "\n",
      "A violent confrontation took place in what appears to be a public area. Multiple individuals are seen involved in a physical fight, with one or more subjects engaged in striking and kicking others. A warning screen at the beginning of the video indicates that the footage may be upsetting to some viewers. This incident could potentially be classified as assault and may be subject to legal investigation depending on the jurisdiction.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "video = cv2.VideoCapture(\"/Users/sameet/Crime GPT/Anomaly-Videos-Part-1/Assault/Assault001_x264.mp4\")\n",
    "\n",
    "# Set the start and end time for trimming\n",
    "start_time = 0  # Start time in seconds\n",
    "end_time = 30  # End time in seconds\n",
    "\n",
    "# Convert time to frame indices\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "start_frame = int(start_time * fps)\n",
    "end_frame = int(end_time * fps)\n",
    "\n",
    "# Read and discard frames until the start frame\n",
    "for _ in range(start_frame):\n",
    "    success, _ = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "# Read and process frames within the desired range\n",
    "base64Frames = []\n",
    "for _ in range(start_frame, end_frame):\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "video.release()\n",
    "print(len(base64Frames), \"frames read.\")\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            \"These are frames from a video that I want to upload. Categorize this video in crime or not crime. If it is a crime generate a description\",\n",
    "            *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "params = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": PROMPT_MESSAGES,\n",
    "    \"max_tokens\": 200,\n",
    "}\n",
    "\n",
    "result = client.chat.completions.create(**params)\n",
    "print(result.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2486 frames read.\n"
     ]
    }
   ],
   "source": [
    "# video = cv2.VideoCapture(\"/Users/sameet/Crime GPT/Anomaly-Videos-Part-1/Assault/Assault001_x264.mp4\")\n",
    "\n",
    "# base64Frames = []\n",
    "# while video.isOpened():\n",
    "#     success, frame = video.read()\n",
    "#     if not success:\n",
    "#         break\n",
    "#     _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "#     base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "# video.release()\n",
    "# print(len(base64Frames), \"frames read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the frames provided, the video appears to capture a violent altercation among several individuals. Given that physical violence is portrayed, this categorizes the video as involving a crime.\n",
      "\n",
      "### Description:\n",
      "The video depicts a violent street altercation involving multiple individuals. The footage shows various phases of the incident, where the individuals are seen engaging in physical combat. One person appears to be beaten by a group, sustaining blows while on the ground. The situation seems intense and potentially harmful, emphasizing the severity of the aggression displayed.\n",
      "\n",
      "Given the nature content, if you decide to upload, ensure you include appropriate warnings and consider the platform’s guidelines to avoid content removal or violations of community standards.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PROMPT_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            \"These are frames from a video that I want to upload. Categorize this video in crime or not crime. If it is a crime generate a description\",\n",
    "            *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "params = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": PROMPT_MESSAGES,\n",
    "    \"max_tokens\": 200,\n",
    "}\n",
    "\n",
    "result = client.chat.completions.create(**params)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 frames read.\n",
      "The video frames depict a violent altercation involving multiple individuals. The sequence shows one or more individuals assaulting another person who appears to be on the ground at some points. Given the nature of the actions displayed in the frames, this situation can be classified as a potential crime, particularly physical assault.\n",
      "\n",
      "### Description:\n",
      "The video captures a street brawl involving multiple assailants attacking an individual. The footage shows one or more people repeatedly striking the victim while they are on the ground, suggesting a violent physical assault. The scenario unfolds rapidly and appears to be unprovoked. It portrays a clear instance of aggression and physical violence, likely resulting in harm to the targeted individual. \n",
      "\n",
      "Viewer discretion is advised due to the graphic nature of the content.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "import openai\n",
    "from IPython.display import display, Image, Audio\n",
    "import time\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "api_key = os.getenv('OPEN_AI_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet('/Users/sameet/Crime GPT/yolov3/yolov3.cfg', '/Users/sameet/Crime GPT/yolov3/yolov3.weights')\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "video = cv2.VideoCapture(\"/Users/sameet/Crime GPT/Anomaly-Videos-Part-1/Assault/Assault001_x264.mp4\")\n",
    "\n",
    "person_detected = False\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = video.read()\n",
    "    if not ret or person_detected:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame, None, fx=0.4, fy=0.4)\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    \n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:  \n",
    "                person_detected = True  \n",
    "                break  \n",
    "        if person_detected:\n",
    "            break \n",
    "\n",
    "    if person_detected:\n",
    "        break  \n",
    "if person_detected:\n",
    "    # Reopen the video file to process frames from the beginning\n",
    "    video = cv2.VideoCapture(\"/Users/sameet/Crime GPT/Anomaly-Videos-Part-1/Assault/Assault001_x264.mp4\")\n",
    "\n",
    "    start_time = 0  \n",
    "    end_time = 30  \n",
    "\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "\n",
    "    for _ in range(start_frame):\n",
    "        success, _ = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "    base64Frames = []\n",
    "    for _ in range(start_frame, end_frame):\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "    video.release()\n",
    "    print(len(base64Frames), \"frames read.\")\n",
    "\n",
    "    PROMPT_MESSAGES = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                \"These are frames from a video that I want to upload. Categorize this video in crime or not crime. If it is a crime generate a description\",\n",
    "                *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": PROMPT_MESSAGES,\n",
    "        \"max_tokens\": 200,\n",
    "    }\n",
    "\n",
    "    result = client.chat.completions.create(**params)\n",
    "    print(result.choices[0].message.content)\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flask in /Users/sameet/Library/Python/3.9/lib/python/site-packages (3.0.3)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: openai in /Users/sameet/Library/Python/3.9/lib/python/site-packages (1.30.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/sameet/Library/Python/3.9/lib/python/site-packages (1.0.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from flask) (7.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=3.6.0->flask) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/sameet/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Downloading opencv_python_headless-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "!pip install flask opencv-python-headless openai python-dotenv\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
